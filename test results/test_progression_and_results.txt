Accuracy/Size	Description
.42 / 100	Largest: (2->4 points bonus for consecutive)(largest 4 words now) 
.43 / 100	"" 
.346 / 500	"" 
0.376 / 500	"" + combined definitions and examples

.26 / 100	Sentence: (2->4 points bonus for consecutive) 
0.352 / 500	"" + combined definitions and examples 


500 tests - largest 4 words, 4 points for consecutive and simplified lesk, added examples to defs
Largest: .362
Sentence: .332
Hybrid: .316
Random: .18, .224, .194, .21, .222, .19, .232, .214, .202, .196 (.205 on 8000 random tests)


Comprehensive (whole set) Test:
Largest 4 words
4->3 Points for Consecutive (bonus) and Simplified Lesk
Examples added to defs

Result: .3476

^Sentence (500): .354
Senrence (Comprehensive): .3630



Looking into comparing context words - actually using the training data to train!
Naive Model:  Tracks the context words seen for every sense id (trained on 75% of the training data)
Gets context for desired test case, and simply scores that context against the whole history of
context words for every sense.  if there is an exception, it reverts to the lesk model
Tested on 25% of the data (1965 tests).  Using "Sentence" based context approach.

Result: .5690
Lesk Fallback Rate: .0758

"" for Largest

Result: .5812			- I didn't expect this to perform better than the sentence approach
Lesk Fallback Rate: .0612

"" For Sentence with Context Defs (non-naive approach):

Result: .5822
Lesk Fallback Rate: 0

"" For Largest with Context Defs (non-naive approach):

Result: -
Lesk Fallback Rate: -

Just realized that all above scores are for if the prediction for the target word is in the list of
possible senses.  Did not take into account that we are supposed to predict all the possible senses

First Kaggle Scores following this limited approach (only one prediction is made)
Sentence Naive: 0.51116
Largest Naive:	0.51876
Largest Adv:	

^ note that these were submitted using training from 75% of the data, not the full set...


Possible improvement: predicting a U?  U happens 194/7860 cases in training = 2.4% of the time, may not
be worth doing, as we may hurt our other 97.6% of our cases by implementing a confidence factor.


New improvements: Score = Score / len(history context), and multi-sense prediction


Testing Sentence - Context - Naive - 25% reserved test set with failing on not predicting all cases correctly
Results: .5089
Lesk Fallback Rate: .0758

Testing ^ " " + score = score / len(history context) (score for a defininition is higher if a higher proportion
of its history is matched by the current context)
Results: .1816
Lesk Fallback Rate:

Testing ^ " " + multi-sense predictions
Results: .1674
Lesk Fallback Rate: .075
Passed Multi Sense (that normally wouldnt be passed): 31
Failed Single Sense (with multi sense prediction): 361

Looking into consecutive scoring bonus for matching between contexts: (5 for consec match, 1 for normal)
Results: .5770 vs .5690
Lesk Fallback Rate: same (.0758)

"" + ignore context words <= 3 in length
Results: .5613 (did worse)

"" + if tar word in context history, + 10 to score
Results: .5582

Just +10 for tar word match
Results: .5751

+5 for tar word match, +3 for consecutive match
Results: .5760 vs .5690


JUST Random instead of Lesk when failing:
Results: .5603 vs .5690

JUST Unknown instead of Lesk when failing:
Results:  .5501 vs .5690

+5/+3 + <= 2 word lengths thrown out of matching:
Sentence: .5730 vs .5760


+5/+3 - 5-skipgram (takes sentence context and reduces it to a span of 10 if it is larger than 10):
Result: .5486

Def context matching b/w sentence based, where everything has been set-ified: .5826



Realized that consecutive matches aren't really in consecutive phrases...
The code is "if x is in y", x can be anywhere in y, so consecutive matches don't mean much, which 
may account for the lack of too big a bump from rewarding consecutive matches above.

This may be happening for Lesk as well, potentially explaining the less than stellar results.

Fixed - sentence naive
Result: .5913

^ with +4 for consecutive, instead of +2
-worse

Lesk with better consecutive scoring: .3643


Found bug where "tar" from the <tar> tag kept appearing in the contextData.
Result after removed: Sentence naive:
Result: .6010
Lesk Fallback: .1013

Falling back to context defs (adv) and then to lesk fallback (since defs outperforms lesk, and we saw
a fallback rate of 10% - which we likely only got about 30-35% of correct, could shoot for 50-60% with adv):
Result: .6046
Context Def Fallback: .1013
Lesk Fallback: 0

"" + skipgram
Result:	.6056
Context Def Fallback: .1323
Lesk Fallback: 0

"" + skipgram with NO filtering out of context words based on pos
Result: .5766
Context Def Fallback: .00051
Lesk Fallback:

"" + skipgram with filtering words less than 2 in length, and "tar" and "/tar"
Skipgram: .5710
Sentence: .5725

- points for consec and word match
Sentence: .6041
Skipgram: .6046

^ "" + remove set-ifying context def approach:
Sentence: .6031
Skipgram: .6046


Reverted to stable state (+2 for consec, double fallback = .6046 for sentence, .6056 for skipgram)
New approach: Scale each score by count(senseid) / sum(count(senseids) for target)
initial (with fallback disabled, so sometimes a 0 score was chosen): 58.12

Sentence (accidently ran on skipgram data): .6041
Skipgram: .6051
Sentence: .6041

Set-ifying context matches, no consecutive points anymore
Skipgram: .5908
Sentence: .5964

Reverted previous change.  Tracking success on each word in my test set:
see "word success breakdown" image

Removed consecutive: consecutive doesn't make any sense, we're removing some words during filtering...
Sentence: .6036
Skipgram: .6046

Going back to try the "largest" words context.  Falling back 13% of the time with skipgram.  maybe largest
words grabs a bigger / more reliable context
Largest: .5959

Sentence: Divide by len history as well as scale by prob
.6071

Ran "all words" again, and turns out its not super slow.  Not fast per average case but has almost no fallbacks.
Jumped the kaggle score to 58%. (new best)
Local test on 25%: .6438

Adding "example" for a sense to the context history for the sense.  Added new metrics that we're tracking
Local Test: .6458 

Added swapping with the next highest score if the next highest score is within .75/.9 of the score
and has a higher proportion of seen senses
.75: .6270 (39 passed swaps, 122 failed ones)
.90: .6346 (10 passed swaps, 56 failed ones)

Testing kaggle with no swaps vs swaps at .95
No swaps: .5756
.95: .5791

Kaggle seems worse with the examples added to hist.  Removing examples:
.95 (examples removed): .57454

Swaps at .95 didnt seem to help.  scrapping this approach too.
tracked how many pass/fail under a low score threshold.  (8 passed under score .001, 7 failed)

New strat: what if we dont filter out adjectives? maybe filtering out some useful words
25% test set: .6346 (vs .6438)

Switched to stopword filtering for much better performance (2 min instead of 20 min to run one test):
Perf: 63.61%

Threshold Investigation:
Avrg Score: .0196
Lowest Score: .0012

	Threshold = .005:
		Below that pass: 55
		Below that fail: 80

	Threshold = .0025:
		Below that pass: 9
		Below that fail: 18

Reverting to majority sense if score below threshold:
t=.0025: had 3 pass, 24 fail.  Did worse this way...

Looking into predicting majority sense or using other methods for words we're scoring poorly on:
Worst words:
simple: 22%
judgement: 25%
solid: 21%

for above words, predicting majority sense
Total perf: 63.40%  vs 63.61%
simple: 0%
judgement: 25%
solid: 7%

for above words, falling back to context defs
Total perf:63.51% vs 63.61%
simple: 0%
judgement: 25%
solid: 21%

for above words, falling back to context defs (now properly scaled for senseid frequency)
Total perf:63.71% vs 63.61%
simple: 33%
judgement: 25%
solid: 28%

Judgement doesn't seem to be affected.  Apparently it's spelled "judgement"
for above words, falling back to context defs (now properly scaled for senseid frequency)
Total perf: 63.66% vs 63.61%
simple: 33%
judgement: 18.75%
solid: 28%

^ above + dividing by length of history glob
Total perf: 63.66% vs 63.61%
simple: 33%
judgement: 18.75%
solid: 28%

Scrapping all of the above.  going to test on whole test set with training on whole test test.  Will then look at 
worst performers, since most things should be passing really well when self-validating...
83% accuracy when trained and tested on itself.  Words that did really poorly:
argument: 63%
begin: 64%
ask: 62%

Stemming now on top of lemmatizing:
62.85%

No stemming or lemmatizing:
63.96% vs 63.61%

Add example to seen history (token and filter it):
no effect on results

Remove len(hist) downscaling:
61.17% vs 63.96%

Synset context history (every token in context history is made into a list of synonyms and put back) - not done for curr_context:
61.17% vs 63.96%

downscale on length of unique history context elements
62.49% vs 63.96%

Not filtering out tokens <= 3 in length, not filtering tokens that are purely numeric
62.13% vs 63.96%

